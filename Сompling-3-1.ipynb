{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание 3 \n",
    "**Выполнили Гайдук Юлия, Панкратова Анна МЖД172**\n",
    "\n",
    "Задание: Взять коллекцию текстов (не менее 100Кб), построить базовые частотные списки, построить список ключевых слов используя алгоритм RAKE, сравнить полученные результаты.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа выполняется на материале произведения Е. И. Рериха [\"Мир Огненный\"](http://www.lib.ru/RERIH/Rerih_Mir1.txt) объемом 2 Мб"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "import pymorphy2\n",
    "from rake_nltk import Metric, Rake\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append(r'C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36-32\\Lib\\site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# скрипт из репо\n",
    "def lemmatize_pm(fname, text):\n",
    "    \"\"\"\n",
    "    fname: название файла, куда будет записан результат лемматизации\n",
    "    text: текст для анализа\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "    with open(fname, 'w', encoding='utf-8') as outfile:\n",
    "        for token in tokens:\n",
    "            parsed = morph.parse(token)\n",
    "            outfile.write(parsed[0].normal_form + ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_text(text):\n",
    "    # Убирает знаки препинания и символы. \n",
    "    text = re.sub(r'[^\\w\\s]','', text)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = text.replace('\\n','')\n",
    "    text = text.replace('\\x14','')\n",
    "    text = text.replace('\\x15','')\n",
    "    text = text.replace('pp','')\n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    # На вход подаётся подготовленный список слов. Возвращает список лемм.\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    normed_text = []\n",
    "    for word in text:\n",
    "        normed_word = morph.parse(word)[0].normal_form\n",
    "        normed_text.append(normed_word)\n",
    "    return normed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Rerih_Mir.txt\") as f:\n",
    "    text = f.read()\n",
    "text = prepare_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_pm('lemmas_pm.txt', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем список стоп-слов\n",
    "with open(\"ru_stop_words.txt\") as f:\n",
    "    stop_words = f.read()\n",
    "stop_words = stop_words.split(\"\\n\")\n",
    "stop_words.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# очищаем текст от стоп-слов\n",
    "with open(\"lemmas_pm.txt\") as f:\n",
    "    lemmas_pm = f.read().split()\n",
    "processed_text = [word for word in lemmas_pm if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_Afreq(text):\n",
    "    # На вход подаётся список элементов.\n",
    "    # Возвращает словарь абсолютных частот, где ключами являются элементы, \n",
    "    # а значениями — количество появлений элемента в тексте.\n",
    "    from collections import Counter\n",
    "    c = Counter()\n",
    "    for word in text:\n",
    "        c.update({word:1})\n",
    "    return dict(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Частотный список\n",
      "Лемма 'огненный' встречается в тексте 2168 раз.\n",
      "Лемма 'мир' встречается в тексте 1705 раз.\n",
      "Лемма 'нужно' встречается в тексте 1291 раз.\n",
      "Лемма 'можно' встречается в тексте 1163 раз.\n",
      "Лемма 'человек' встречается в тексте 1144 раз.\n",
      "Лемма 'дух' встречается в тексте 980 раз.\n",
      "Лемма 'такой' встречается в тексте 966 раз.\n",
      "Лемма 'огонь' встречается в тексте 883 раз.\n",
      "Лемма 'сердце' встречается в тексте 881 раз.\n",
      "Лемма 'который' встречается в тексте 881 раз.\n",
      "Лемма 'каждый' встречается в тексте 826 раз.\n",
      "Лемма 'энергия' встречается в тексте 813 раз.\n",
      "Лемма 'когда' встречается в тексте 809 раз.\n",
      "Лемма 'тонкий' встречается в тексте 747 раз.\n",
      "Лемма 'самый' встречается в тексте 710 раз.\n",
      "Лемма 'сознание' встречается в тексте 691 раз.\n",
      "Лемма 'высокий' встречается в тексте 664 раз.\n",
      "Лемма 'свой' встречается в тексте 643 раз.\n",
      "Лемма 'мысль' встречается в тексте 634 раз.\n",
      "Лемма 'явление' встречается в тексте 614 раз.\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "Afreq_text = get_Afreq(processed_text)\n",
    "Afreq_list_sorted = sorted(Afreq_text.items(), key=itemgetter(-1), reverse=True)\n",
    "print('Частотный список')\n",
    "for tpl in Afreq_list_sorted[:20]:\n",
    "    print(\"Лемма '{}' встречается в тексте {} раз.\".format(tpl[0], tpl[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь применим алгоритм RAKE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_text = text.split()\n",
    "norm_text = normalize_text(split_text)\n",
    "prep_text = \" \".join(norm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее часто встречающиеся словосочетания длины 2 по лемматизированному тексту:\n",
      "огненный дух\n",
      "дух человек\n",
      "только человек\n",
      "огненный сознание\n",
      "сердце огненный\n",
      "огненный сердце\n",
      "сознание человек\n",
      "огонь дух\n",
      "огненный мир\n",
      "мир огненный\n"
     ]
    }
   ],
   "source": [
    "# Лемматизированный текст, сортировка по частоте, длина 2\n",
    "r = Rake(language=\"russian\", stopwords=stop_words, max_length=2, ranking_metric=Metric.WORD_FREQUENCY)\n",
    "r.extract_keywords_from_text(prep_text)\n",
    "freq_2 = r.get_ranked_phrases() \n",
    "print(\"Наиболее часто встречающиеся словосочетания длины 2 по лемматизированному тексту:\")\n",
    "for item in freq_2[:10]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее часто встречающиеся словосочетания длины 3 по лемматизированному тексту:\n",
      "мир огненный человек\n",
      "огненный мир нужно\n",
      "мир огненный нужно\n",
      "мир огненный можно\n",
      "мир огненный сердце\n",
      "энергия огненный мир\n",
      "огненный мир когда\n",
      "мир огненный когда\n",
      "земной мир огненный\n",
      "мир огненный ибо\n"
     ]
    }
   ],
   "source": [
    "# Лемматизированный текст, сортировка по частоте, длина 3\n",
    "r = Rake(language=\"russian\", stopwords=stop_words, max_length=3, ranking_metric=Metric.WORD_FREQUENCY)\n",
    "r.extract_keywords_from_text(prep_text)\n",
    "freq_3 = r.get_ranked_phrases() # Сортировка по частоте, длина 3\n",
    "print(\"Наиболее часто встречающиеся словосочетания длины 3 по лемматизированному тексту:\")\n",
    "for item in freq_3[:10]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее часто встречающиеся словосочетания длины 2 по нелемматизированному тексту:\n",
      "может сердце\n",
      "будет нужно\n",
      "только могут\n",
      "жизни может\n",
      "сознание может\n",
      "нужно сердце\n",
      "можно будет\n",
      "огонь может\n",
      "огненного нужно\n",
      "только тонкого\n"
     ]
    }
   ],
   "source": [
    "# Нелемматизированный текст, сортировка по частоте, длина 2\n",
    "r = Rake(language=\"russian\", stopwords=stop_words, max_length=2, ranking_metric=Metric.WORD_FREQUENCY)\n",
    "r.extract_keywords_from_text(text)\n",
    "freq_2_simple = r.get_ranked_phrases() # Сортировка по частоте, длина 2\n",
    "print(\"Наиболее часто встречающиеся словосочетания длины 2 по нелемматизированному тексту:\")\n",
    "for item in freq_2_simple[:10]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее часто встречающиеся словосочетания длины 3 по нелемматизированному тексту:\n",
      "нужно строительство можно\n",
      "может остановиться можно\n",
      "нужно только вспомнить\n",
      "только можно надеяться\n",
      "может уменьшиться только\n",
      "когда нужно напомнить\n",
      "только можно усвоить\n",
      "будет нужно\n",
      "нужно говорить ибо\n",
      "нужно растерзание духа\n"
     ]
    }
   ],
   "source": [
    "# Нелемматизированный текст, сортировка по частоте, длина 2\n",
    "r = Rake(language=\"russian\", stopwords=stop_words, max_length=3, ranking_metric=Metric.WORD_FREQUENCY)\n",
    "r.extract_keywords_from_text(text)\n",
    "freq_3_simple = r.get_ranked_phrases() # Сортировка по частоте, длина 3\n",
    "print(\"Наиболее часто встречающиеся словосочетания длины 3 по нелемматизированному тексту:\")\n",
    "for item in freq_3_simple[:10]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
